# ü§ñ AVSI ‚Äî Artificial Vision Stacking Inspection
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)]()
[![Framework](https://img.shields.io/badge/Framework-PyTorch-orange.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)]()
[![Status](https://img.shields.io/badge/Build-Stable-success.svg)]()

## üìÑ Descripci√≥n t√©cnica del proyecto
**AVSI (Artificial Vision Stacking Inspection)** es un sistema de **visi√≥n por computador e inteligencia artificial** desarrollado para inspeccionar autom√°ticamente el apilamiento de pa√±ales en l√≠neas de producci√≥n industrial.  
Mediante una **red neuronal convolucional (ResNet-18)** y t√©cnicas de **transfer learning**, el sistema identifica apilamientos correctos e incorrectos en tiempo real, con una precisi√≥n superior al 94% y una velocidad mayor a 30 FPS.  
El proyecto integra un pipeline completo de datos, entrenamiento y evaluaci√≥n con una interfaz gr√°fica en **Streamlit**, orientado a la mejora de calidad y reducci√≥n de errores humanos en procesos de manufactura.

##  Descripci√≥n grafica 
<img width="1536" height="1024" alt="ChatGPT Image Oct 16, 2025, 06_36_02 PM" src="https://github.com/user-attachments/assets/ae897a3e-3e11-4417-abfd-d7236dca71e2" />

**Fuente:** AVSI / Generado por ChatGPT


**Profesor:** PhD. Gladys Villegas  
**Autores:** Francisco Javier Estupi√±√°n Andrade / David Alejandro Narv√°ez Mej√≠a  

---

## üìö Tabla de Contenidos
1. [Descripci√≥n del Problema](#-descripci√≥n-del-problema)  
2. [Dataset](#-dataset)  
3. [Metodolog√≠a](#-metodolog√≠a)  
4. [Resultados](#-resultados)  
5. [Instalaci√≥n y Uso](#-instalaci√≥n-y-uso)  
6. [Interfaz de Usuario](#-interfaz-de-usuario)  
7. [Estructura del Proyecto](#-estructura-del-proyecto)  
8. [Consideraciones √âticas](#-consideraciones-√©ticas)  
9. [Autores y Contribuciones](#-autores-y-contribuciones)  
10. [Licencia](#-licencia)  
11. [Agradecimientos y Referencias](#-agradecimientos-y-referencias)  

---

## üß© Descripci√≥n del Problema

El problema central se centra en la **verificaci√≥n visual del apilamiento de pa√±ales antes del empaque**, una tarea que actualmente depende de la observaci√≥n humana y expone a los operarios a **riesgos ergon√≥micos y as√©pticos**, adem√°s de requerir **decisiones en tiempo real** dentro de entornos de alta producci√≥n.  

En este contexto, la tendencia industrial apunta hacia soluciones basadas en **Deep Learning**, donde las **redes neuronales convolucionales (CNN)** y los modelos de detecci√≥n modernos ‚Äîcomo **YOLO** o **EfficientDet**‚Äî han demostrado superar ampliamente a la visi√≥n cl√°sica en t√©rminos de **precisi√≥n, robustez y velocidad**, consolid√°ndose como la alternativa m√°s competitiva para l√≠neas automatizadas.

Sin embargo, la adopci√≥n pr√°ctica enfrenta desaf√≠os asociados a la **escasez de datos**, la **sensibilidad a variaciones de dominio** (cambios en iluminaci√≥n, posici√≥n o textura) y la **latencia de procesamiento**. Frente a ello, la literatura especializada recomienda:
- **Aumento de datos (Data Augmentation)** para mejorar la generalizaci√≥n.  
- **Anotaci√≥n activa (Active Learning)** que optimiza el etiquetado en datasets reducidos.  
- **Validaci√≥n mediante m√©tricas estandarizadas** como *mAP*, *IoU* y *F1-score*.  
- **Implementaci√≥n en entornos de Edge Computing**, donde el tiempo de respuesta es cr√≠tico.

Finalmente, se evidencia un **vac√≠o de investigaci√≥n y aplicaci√≥n** en el control de calidad de **productos higi√©nicos**, como los pa√±ales, donde la **falta de conjuntos de datos p√∫blicos** y la necesidad de **protocolos de validaci√≥n en entornos industriales controlados** ‚Äîespecialmente con datasets limitados, como el de 1 000 im√°genes desarrollado en este proyecto‚Äî representan una **oportunidad estrat√©gica para la innovaci√≥n acad√©mica e industrial** en visi√≥n artificial aplicada a manufactura inteligente.

### ¬øQui√©nes son los usuarios objetivo?
Ingenieros de control de calidad, t√©cnicos de automatizaci√≥n y operadores de l√≠neas industriales dentro del sector de productos higi√©nicos, alimentos y empaques autom√°ticos.

<img width="974" height="454" alt="image" src="https://github.com/user-attachments/assets/cd1afd63-5ce9-4a90-8c6d-29b7fb341f5e" />

**Fuente:** Visi√≥n Artificial Apilamiento / FUENTE: EVA ENGINEERING 

---

## üìä Dataset

### Descripci√≥n de los datos utilizados
El sistema emplea **dos datasets propios** desarrollados por **EVA ENGINEERING S.A.**:
| Dataset | N¬∫ de Im√°genes | Descripci√≥n | Prop√≥sito |
|----------|----------------|--------------|------------|
| Dataset 1 | 100 | Im√°genes originales capturadas en planta bajo distintas condiciones. | Baseline inicial. |
| Dataset 2 | 1 000 | Dataset ampliado con *data augmentation*. | Entrenamiento final optimizado. |

<img width="839" height="614" alt="image" src="https://github.com/user-attachments/assets/d93db7fb-8a93-47b0-84dc-e30338161084" />

**Fuente:** Pilas Agrupadas / FUENTE: VC_ResNet_18_Rev_0.ipynb

### Fuente y licencia de los datos
- **Procedencia:** EVA ENGINEERING S.A. (c√≥digo IN34-EVA1200 REV 01).  
- **Licencia:** Uso acad√©mico y prototipado interno.  
- **Condici√≥n:** Prohibida su distribuci√≥n sin autorizaci√≥n.  

### Caracter√≠sticas principales
- Formato: JPG, resoluci√≥n 640√ó640 px  
- Clases: `good_stack` / `bad_stack`  
- Balanceo: Estratificado 70/15/15  
- Calidad: Im√°genes industriales RGB 8-bit  

---

## üß† Metodolog√≠a

## üß≠ Flujo de Proceso para el Dise√±o e Implementaci√≥n de un Proyecto de Visi√≥n por Computador

Arquitectura Vision Artificial  
<img width="1183" height="407" alt="ARQUITECTURA VISION" src="https://github.com/user-attachments/assets/78f47308-047e-4eb1-bfd8-fb4b39495f5b" />

**Fuente:** Proceso de proyectos Visi√≥n por Computador / FUENTE: Machine-Vision-Systems-Design A3 VISION

<img width="1414" height="854" alt="image" src="https://github.com/user-attachments/assets/ad4e652e-f2bd-4bf6-bdcd-1490adcbf762" />

**Fuente:** Proceso de proyectos Visi√≥n por Computador / FUENTE: Machine-Vision-Systems-Design A3 VISION

El siguiente flujo describe paso a paso el proceso t√≠pico para dise√±ar e implementar un sistema de **visi√≥n por computador (Machine Vision, MV)**.  
Acompa√±a al diagrama **‚ÄúTypical Design Sequence‚Äù**, y sirve como gu√≠a pr√°ctica para la selecci√≥n, integraci√≥n y validaci√≥n de los componentes √≥pticos, electr√≥nicos y de software involucrados en un sistema industrial de visi√≥n.

---

## 1Ô∏è‚É£ Definir el tipo de sistema de visi√≥n (MV)
**Prop√≥sito:** elegir la arquitectura general (smart camera, c√°mara atada a controlador, PC-based).  
**Entradas:**
- Objetivos de calidad  
- Tasa de producci√≥n  
- Espacio disponible  
- Presupuesto  
- Integraci√≥n con PLC o robot  

**Salidas:** arquitectura elegida, cantidad de c√°maras, esquema de disparo (*trigger*) y sincronizaci√≥n.  

> üí° **Nota de proyecto:** Si existen m√∫ltiples vistas o alta complejidad (mediciones precisas o uso de Machine Learning), suele ser m√°s conveniente un sistema **PC-based** por su flexibilidad y capacidad de procesamiento.

---

## 2Ô∏è‚É£ Seleccionar la(s) c√°mara(s)
**Pasos:**
- Definir el **campo de visi√≥n (FOV)** con margen (10‚Äì20% de *overscan*).  
- Calcular la **resoluci√≥n espacial requerida (mm/px)** a partir del tama√±o m√≠nimo a medir y la cantidad de p√≠xeles necesarios para cubrir la caracter√≠stica.  
- Validar **velocidad** (fps y tiempo de exposici√≥n para que el desenfoque por movimiento sea ‚â§ 1 px).  
- Evaluar tipo de obturador (preferiblemente *global*), interfaz (GigE, USB3, CoaXPress).  

**Salidas:** modelo de c√°mara, resoluci√≥n, fps, tipo de obturador e interfaz.

> üí° **Nota de proyecto:** Si la l√≠nea se mueve r√°pido, priorizar *global shutter* y tiempos de exposici√≥n cortos.

---

## 3Ô∏è‚É£ Seleccionar el lente
**Pasos:**
- Calcular la **magnificaci√≥n** (sensor/FOV) y la distancia de trabajo.  
- Seleccionar la **focal** que cumpla con el FOV deseado a la distancia real y asegure resoluci√≥n √≥ptica ‚â• frecuencia de Nyquist del p√≠xel.  
- Verificar **tipo de montura** (C/CS/S) y nivel de distorsi√≥n aceptable.  
- Considerar lentes **telec√©ntricos** cuando la precisi√≥n dimensional es cr√≠tica.  

**Salidas:** longitud focal, distancia de trabajo, tipo de montura y requisitos de resoluci√≥n √≥ptica.

---

## 4Ô∏è‚É£ Verificar c√°mara y lente
**Pruebas en banco:**
- Enfocar al FOV objetivo.  
- Medir la distancia de trabajo real.  
- Comprobar resoluci√≥n en el centro y esquinas mediante una diana de calibraci√≥n.  
- Revisar distorsi√≥n √≥ptica.  

**Criterio de salida:** cumplimiento del FOV, nitidez y resoluci√≥n en todo el campo visual.

---

## 5Ô∏è‚É£ Seleccionar la t√©cnica/fuente de iluminaci√≥n
**Pasos:**
- Partir de la **caracter√≠stica que requiere contraste** (bordes, alineaci√≥n, huecos, textura, etc.).  
- Elegir **geometr√≠a √≥ptica** (backlight, front light difusa, darkfield o brightfield).  
- Definir **difusividad, longitud de onda, polarizaci√≥n**, y modo de operaci√≥n (continua o estrobosc√≥pica).  

**Salida:** modelo y geometr√≠a de iluminaci√≥n + m√©todo de fijaci√≥n mec√°nica.

> üí° **Nota de proyecto:**  
> - Para contornos y *gaps*, un **backlight difuso** produce siluetas limpias.  
> - Para inspecci√≥n de textura o superficie, se recomienda **front-light difusa**.

---

## 6Ô∏è‚É£ Verificar la imagen (Imaging)
**Montaje preliminar:**
- Capturar un lote representativo de muestras.  
- Evaluar exposici√≥n, uniformidad, contraste, reflejos, sombras y **SNR (Signal-to-Noise Ratio)**.  
- Ajustar √°ngulos, altura y ganancia.  

**Integrar el EDA de im√°genes:**
- Histogramas de brillo y contraste.  
- Medici√≥n de desenfoque (varianza del Laplaciano).  
- Distribuci√≥n de tama√±os y balance de clases.  
- Detecci√≥n de duplicados (*pHash*) y *outliers*.  

**Salidas:**  
- Par√°metros finales de c√°mara/iluminaci√≥n.  
- Plan de normalizaci√≥n (resize, recorte, normalizaci√≥n RGB).  
- Plan de *augmentation* seg√∫n la variabilidad real observada.

---

## 7Ô∏è‚É£ Dise√±ar el procesamiento de imagen
**Definir el pipeline t√©cnico** a partir del an√°lisis EDA y los requisitos funcionales:

1. **Preprocesamiento:** redimensionamiento, normalizaci√≥n, reducci√≥n de ruido.  
2. **Localizaci√≥n/segmentaci√≥n:** bordes, umbrales adaptativos, morfolog√≠a, o modelo CNN/Detecci√≥n.  
3. **C√°lculos y mediciones:** alineaci√≥n, separaciones, conteo, m√©tricas geom√©tricas.  
4. **Clasificaci√≥n:** OK/Defecto o m√©tricas de calidad.  

**Selecci√≥n de herramientas:**
- M√©todos cl√°sicos (OpenCV, filtros, morfolog√≠a).  
- Deep Learning (CNN, CNN+Transformers) seg√∫n la separabilidad visual detectada (PCA/embeddings).

**Definir KPIs de desempe√±o:** precisi√≥n, recall, F1-score, tiempo de ciclo.  
**Protocolo de pruebas:** FAT (Factory Acceptance Test).  

**Salidas:**  
- Diagrama de bloques del pipeline.  
- Par√°metros iniciales de procesamiento.  
- Dataset curado y dividido (train / val / test).

---

## 8Ô∏è‚É£ Implementar
**Actividades:**
- Integraci√≥n con PLC, robot o HMI (protocolos EtherNet/IP, Profinet, etc.).  
- Calibraciones: intr√≠nseca y mano-ojo si hay medidas en mm.  
- Construcci√≥n mec√°nica, cableado, seguridad y receta de producto.  
- Ejecuci√≥n de **pruebas FAT** en taller con criterios definidos:  
  tasas de falsos ¬± m√°ximas, tiempo de ciclo, estabilidad y repetibilidad.

---

## 9Ô∏è‚É£ Desplegar
**Fase de puesta en marcha:**
- Instalaci√≥n y **SAT (Site Acceptance Test)** en planta.  
- Validaci√≥n en condiciones reales de producci√≥n.  
- Capacitaci√≥n a operadores y personal de mantenimiento.  
- Entrega de documentaci√≥n t√©cnica:  
  par√°metros de configuraci√≥n, planos el√©ctricos/mec√°nicos, *backups* y manuales.  

**Plan de soporte y mejora continua:**
- Registro de im√°genes y logs de inspecci√≥n.  
- Reentrenos peri√≥dicos y ajuste de umbrales adaptativos.  
- Mantenimiento de la iluminaci√≥n, c√°maras y √≥ptica.

---

‚úÖ **Resumen:**  
El proceso completo abarca desde la definici√≥n de la arquitectura hasta el despliegue en planta, asegurando que el sistema de visi√≥n cumpla los requisitos t√©cnicos, productivos y de calidad bajo condiciones reales de operaci√≥n industrial.


### Tipo de modelo utilizado y justificaci√≥n
Se seleccion√≥ una **ResNet-18** con *transfer learning* desde **ImageNet**, por su equilibrio entre precisi√≥n, velocidad y eficiencia en entornos industriales.  
Se reemplaz√≥ la capa final (`fc`) por una densa binaria para clasificaci√≥n y se aplic√≥ *dropout (0.3)* para regularizaci√≥n.

<img width="975" height="373" alt="image" src="https://github.com/user-attachments/assets/62d7545b-96d5-48e5-9c09-00436556dec0" />

**Fuente:** [CS231n ‚Äî Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/)

### Preprocesamiento aplicado
- Redimensionamiento a 224√ó224 px  
- Normalizaci√≥n RGB con medias de ImageNet  
- Data augmentation: rotaciones ¬±15¬∞, flips, jitter de brillo y contraste  
- Balanceo mediante **WeightedRandomSampler**

### T√©cnicas de optimizaci√≥n empleadas
- Grid Search sobre hiperpar√°metros (`lr`, `batch_size`, `dropout`, `optimizer`)  
- Regularizaci√≥n por *early stopping* y *weight decay*  
- Entrenamiento con **Adam (lr=0.0005)** y validaci√≥n cruzada  

### M√©tricas de evaluaci√≥n seleccionadas
- **Accuracy**, **F1-Score**, **mAP (mean Average Precision)**  
- **mIoU (Intersection over Union)**  
- **FPS (velocidad de inferencia)**

---

## üìà Resultados

| Dataset | Accuracy | F1-Score | mAP | mIoU | FPS |
|----------|-----------|-----------|------|------|------|
| Dataset 1 (100 im√°genes) | 0.89 | 0.90 | 0.91 | 0.76 | 110 |
| Dataset 2 (1000 im√°genes) | **0.944** | **0.945** | **0.984** | **0.870** | **144** |

**Comparaci√≥n:**  
La expansi√≥n del dataset y la optimizaci√≥n de hiperpar√°metros incrementaron la precisi√≥n en **+1.2 %** y la estabilidad del modelo, manteniendo tiempo real (>30 FPS).

## RESULTADOS CON DATASET 1 - 100 imagenes 
<img width="675" height="746" alt="image" src="https://github.com/user-attachments/assets/459e6f7c-1206-4c6f-a187-bbca281666a6" />



## RESULTADOS CON DATASET 2 - 1000 imagenes
<img width="659" height="752" alt="image" src="https://github.com/user-attachments/assets/2c8b1897-42e5-4c0f-8c1d-f1b6290ec4fc" />

---
## ‚öñÔ∏è  Consideraciones Eticas 

Este documento presenta un an√°lisis exhaustivo de las **consideraciones √©ticas** inherentes al dise√±o, despliegue y operaci√≥n del sistema **AVSI (Artificial Vision Stacking Inspection)**.  
El an√°lisis se alinea con los principios de **IA Responsable (Responsible AI)**, evaluando los riesgos y proponiendo estrategias de mitigaci√≥n desde una perspectiva sociot√©cnica.

---

## 1Ô∏è‚É£ An√°lisis de Sesgos y Generalizaci√≥n

En el contexto de AVSI, el an√°lisis de sesgos se enfoca en los **sesgos t√©cnicos y de representaci√≥n**, que afectan la robustez y fiabilidad del modelo.

### üîπ Sesgo de Representaci√≥n
El dataset base fue capturado bajo un conjunto limitado de condiciones (iluminaci√≥n, planta, configuraci√≥n de c√°mara).  
Esto genera un **sesgo de muestreo** que limita la capacidad de **generalizaci√≥n** del modelo ante entornos distintos, fen√≥meno conocido como *domain shift*.

### üîπ Sesgo de Medici√≥n
Diferencias sistem√°ticas entre los datos de entrenamiento y el entorno real (calibraci√≥n de c√°mara, compresi√≥n de video, desenfoque por movimiento) introducen un **sesgo de medici√≥n** que puede degradar el rendimiento predictivo.

### üîπ Impacto Predictivo
Estos sesgos se manifiestan como un aumento de **falsos positivos (FPR)** o **falsos negativos (FNR)**.  
Por ejemplo, un modelo entrenado solo con buena iluminaci√≥n podr√≠a fallar durante turnos nocturnos, reduciendo su confiabilidad.

### üîπ Grupos Afectados
- Exceso de falsos positivos ‚Üí fatiga por alertas y sobrecarga cognitiva en operarios.  
- Exceso de falsos negativos ‚Üí riesgos para el consumidor y evaluaciones injustas del personal de calidad.

---

## 2Ô∏è‚É£ Equidad y Fairness Operativo

Dado que AVSI inspecciona objetos, la **equidad** se redefine como **consistencia operativa del rendimiento**.

### üîπ Definici√≥n de Equidad Operativa
Un sistema equitativo mantiene m√©tricas estables (precisi√≥n, F1-score) sin importar turno, l√≠nea, lote o supervisor.

### üîπ M√©tricas de Evaluaci√≥n
Se auditan m√©tricas de error (FPR, FNR) de forma desagregada por variables operativas.  
Diferencias notables entre turnos o l√≠neas indican inequidad operativa.

### üîπ Estrategias de Mitigaci√≥n
- **Data Augmentation:** simular variabilidad de dominio (brillo, contraste, ruido).  
- **Muestreo Estratificado:** asegurar representaci√≥n de condiciones minoritarias o complejas.

---
## ‚öôÔ∏è ### Requisitos del sistema 

| Requisito | Descripci√≥n |
|------------|-------------|
| **Sistema Operativo** | Windows 10/11, macOS o Linux |
| **Navegador Compatible** | Google Chrome, Edge o Firefox |
| **Conectividad** | Internet estable |
| **Formatos admitidos** | `.jpg` o `.png` |
| **Tama√±o m√°ximo de archivo** | 200 MB |
| **Dependencias principales** | `streamlit`, `opencv-python`, `tensorflow`, `numpy`, `plotly` |

### Instalaci√≥n
```bash
git clone https://github.com/usuario/AVSI.git
cd AVSI
pip install -r requirements.txt
# 1Ô∏è‚É£ Procesar datos
python -m src.data_processing

# 2Ô∏è‚É£ Entrenar el modelo
python -m src.train --epochs 10 --lr 0.0005

# 3Ô∏è‚É£ Evaluar resultados
python -m src.evaluate
[10/10] train_acc=0.94 | val_acc=0.945
Matriz de confusi√≥n y m√©tricas guardadas en /results/metrics/
