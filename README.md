# ü§ñ AVSI ‚Äî Artificial Vision Stacking Inspection
[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)]()
[![Framework](https://img.shields.io/badge/Framework-PyTorch-orange.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)]()
[![Status](https://img.shields.io/badge/Build-Stable-success.svg)]()

## üìÑ Descripci√≥n t√©cnica del proyecto
**AVSI (Artificial Vision Stacking Inspection)** es un sistema de **visi√≥n por computador e inteligencia artificial** desarrollado para inspeccionar autom√°ticamente el apilamiento de pa√±ales en l√≠neas de producci√≥n industrial.  
Mediante una **red neuronal convolucional (ResNet-18)** y t√©cnicas de **transfer learning**, el sistema identifica apilamientos correctos e incorrectos en tiempo real, con una precisi√≥n superior al 94% y una velocidad mayor a 30 FPS.  
El proyecto integra un pipeline completo de datos, entrenamiento y evaluaci√≥n con una interfaz gr√°fica en **Streamlit**, orientado a la mejora de calidad y reducci√≥n de errores humanos en procesos de manufactura.

##  Descripci√≥n grafica 
<img width="1536" height="1024" alt="ChatGPT Image Oct 16, 2025, 06_36_02 PM" src="https://github.com/user-attachments/assets/ae897a3e-3e11-4417-abfd-d7236dca71e2" />
**Fuente:** [AVSI / Generado por ChatGPT]


**Profesor:** PhD. Gladys Villegas  
**Autores:** Francisco Javier Estupi√±√°n Andrade / David Alejandro Narv√°ez Mej√≠a  

---

## üìö Tabla de Contenidos
1. [Descripci√≥n del Problema](#-descripci√≥n-del-problema)  
2. [Dataset](#-dataset)  
3. [Metodolog√≠a](#-metodolog√≠a)  
4. [Resultados](#-resultados)  
5. [Instalaci√≥n y Uso](#-instalaci√≥n-y-uso)  
6. [Interfaz de Usuario](#-interfaz-de-usuario)  
7. [Estructura del Proyecto](#-estructura-del-proyecto)  
8. [Consideraciones √âticas](#-consideraciones-√©ticas)  
9. [Autores y Contribuciones](#-autores-y-contribuciones)  
10. [Licencia](#-licencia)  
11. [Agradecimientos y Referencias](#-agradecimientos-y-referencias)  

---

## üß© Descripci√≥n del Problema

### ¬øQu√© problema resuelve el proyecto?
El sistema AVSI automatiza el proceso de **inspecci√≥n del apilamiento de pa√±ales**, que normalmente se realiza de forma manual. Este proceso es propenso a errores humanos, inconsistencias visuales y riesgos ergon√≥micos.  

### ¬øPor qu√© es importante?
En entornos industriales de alta velocidad, incluso peque√±as fallas de apilamiento pueden generar p√©rdidas econ√≥micas significativas, rechazos de producto y fallas en el empaque final. AVSI permite detectar estos defectos en tiempo real con alta precisi√≥n, reduciendo costos y mejorando la trazabilidad de calidad.  

### ¬øQui√©nes son los usuarios objetivo?
Ingenieros de control de calidad, t√©cnicos de automatizaci√≥n y operadores de l√≠neas industriales dentro del sector de productos higi√©nicos, alimentos y empaques autom√°ticos.

---

## üìä Dataset

### Descripci√≥n de los datos utilizados
El sistema emplea **dos datasets propios** desarrollados por **EVA ENGINEERING S.A.**:
| Dataset | N¬∫ de Im√°genes | Descripci√≥n | Prop√≥sito |
|----------|----------------|--------------|------------|
| Dataset 1 | 100 | Im√°genes originales capturadas en planta bajo distintas condiciones. | Baseline inicial. |
| Dataset 2 | 1 000 | Dataset ampliado con *data augmentation*. | Entrenamiento final optimizado. |

<img width="839" height="614" alt="image" src="https://github.com/user-attachments/assets/d93db7fb-8a93-47b0-84dc-e30338161084" />


### Fuente y licencia de los datos
- **Procedencia:** EVA ENGINEERING S.A. (c√≥digo IN34-EVA1200 REV 01).  
- **Licencia:** Uso acad√©mico y prototipado interno.  
- **Condici√≥n:** Prohibida su distribuci√≥n sin autorizaci√≥n.  

### Caracter√≠sticas principales
- Formato: JPG, resoluci√≥n 640√ó640 px  
- Clases: `good_stack` / `bad_stack`  
- Balanceo: Estratificado 70/15/15  
- Calidad: Im√°genes industriales RGB 8-bit  

---

## üß† Metodolog√≠a

##  Arquitectura Vision Artificial  
<img width="1183" height="407" alt="ARQUITECTURA VISION" src="https://github.com/user-attachments/assets/78f47308-047e-4eb1-bfd8-fb4b39495f5b" />
**Fuente:** Proceso de proyectos Visi√≥n por Computador / FUENTE: Machine-Vision-Systems-Design A3 VISION

## üß≠ Flujo de Proceso para el Dise√±o e Implementaci√≥n de un Proyecto de Visi√≥n por Computador

<img width="1414" height="854" alt="image" src="https://github.com/user-attachments/assets/ad4e652e-f2bd-4bf6-bdcd-1490adcbf762" />
**Fuente:** Proceso de proyectos Visi√≥n por Computador / FUENTE: Machine-Vision-Systems-Design A3 VISION

El siguiente flujo describe paso a paso el proceso t√≠pico para dise√±ar e implementar un sistema de **visi√≥n por computador (Machine Vision, MV)**.  
Acompa√±a al diagrama **‚ÄúTypical Design Sequence‚Äù**, y sirve como gu√≠a pr√°ctica para la selecci√≥n, integraci√≥n y validaci√≥n de los componentes √≥pticos, electr√≥nicos y de software involucrados en un sistema industrial de visi√≥n.

---

## 1Ô∏è‚É£ Definir el tipo de sistema de visi√≥n (MV)
**Prop√≥sito:** elegir la arquitectura general (smart camera, c√°mara atada a controlador, PC-based).  
**Entradas:**
- Objetivos de calidad  
- Tasa de producci√≥n  
- Espacio disponible  
- Presupuesto  
- Integraci√≥n con PLC o robot  

**Salidas:** arquitectura elegida, cantidad de c√°maras, esquema de disparo (*trigger*) y sincronizaci√≥n.  

> üí° **Nota de proyecto:** Si existen m√∫ltiples vistas o alta complejidad (mediciones precisas o uso de Machine Learning), suele ser m√°s conveniente un sistema **PC-based** por su flexibilidad y capacidad de procesamiento.

---

## 2Ô∏è‚É£ Seleccionar la(s) c√°mara(s)
**Pasos:**
- Definir el **campo de visi√≥n (FOV)** con margen (10‚Äì20% de *overscan*).  
- Calcular la **resoluci√≥n espacial requerida (mm/px)** a partir del tama√±o m√≠nimo a medir y la cantidad de p√≠xeles necesarios para cubrir la caracter√≠stica.  
- Validar **velocidad** (fps y tiempo de exposici√≥n para que el desenfoque por movimiento sea ‚â§ 1 px).  
- Evaluar tipo de obturador (preferiblemente *global*), interfaz (GigE, USB3, CoaXPress).  

**Salidas:** modelo de c√°mara, resoluci√≥n, fps, tipo de obturador e interfaz.

> üí° **Nota de proyecto:** Si la l√≠nea se mueve r√°pido, priorizar *global shutter* y tiempos de exposici√≥n cortos.

---

## 3Ô∏è‚É£ Seleccionar el lente
**Pasos:**
- Calcular la **magnificaci√≥n** (sensor/FOV) y la distancia de trabajo.  
- Seleccionar la **focal** que cumpla con el FOV deseado a la distancia real y asegure resoluci√≥n √≥ptica ‚â• frecuencia de Nyquist del p√≠xel.  
- Verificar **tipo de montura** (C/CS/S) y nivel de distorsi√≥n aceptable.  
- Considerar lentes **telec√©ntricos** cuando la precisi√≥n dimensional es cr√≠tica.  

**Salidas:** longitud focal, distancia de trabajo, tipo de montura y requisitos de resoluci√≥n √≥ptica.

---

## 4Ô∏è‚É£ Verificar c√°mara y lente
**Pruebas en banco:**
- Enfocar al FOV objetivo.  
- Medir la distancia de trabajo real.  
- Comprobar resoluci√≥n en el centro y esquinas mediante una diana de calibraci√≥n.  
- Revisar distorsi√≥n √≥ptica.  

**Criterio de salida:** cumplimiento del FOV, nitidez y resoluci√≥n en todo el campo visual.

---

## 5Ô∏è‚É£ Seleccionar la t√©cnica/fuente de iluminaci√≥n
**Pasos:**
- Partir de la **caracter√≠stica que requiere contraste** (bordes, alineaci√≥n, huecos, textura, etc.).  
- Elegir **geometr√≠a √≥ptica** (backlight, front light difusa, darkfield o brightfield).  
- Definir **difusividad, longitud de onda, polarizaci√≥n**, y modo de operaci√≥n (continua o estrobosc√≥pica).  

**Salida:** modelo y geometr√≠a de iluminaci√≥n + m√©todo de fijaci√≥n mec√°nica.

> üí° **Nota de proyecto:**  
> - Para contornos y *gaps*, un **backlight difuso** produce siluetas limpias.  
> - Para inspecci√≥n de textura o superficie, se recomienda **front-light difusa**.

---

## 6Ô∏è‚É£ Verificar la imagen (Imaging)
**Montaje preliminar:**
- Capturar un lote representativo de muestras.  
- Evaluar exposici√≥n, uniformidad, contraste, reflejos, sombras y **SNR (Signal-to-Noise Ratio)**.  
- Ajustar √°ngulos, altura y ganancia.  

**Integrar el EDA de im√°genes:**
- Histogramas de brillo y contraste.  
- Medici√≥n de desenfoque (varianza del Laplaciano).  
- Distribuci√≥n de tama√±os y balance de clases.  
- Detecci√≥n de duplicados (*pHash*) y *outliers*.  

**Salidas:**  
- Par√°metros finales de c√°mara/iluminaci√≥n.  
- Plan de normalizaci√≥n (resize, recorte, normalizaci√≥n RGB).  
- Plan de *augmentation* seg√∫n la variabilidad real observada.

---

## 7Ô∏è‚É£ Dise√±ar el procesamiento de imagen
**Definir el pipeline t√©cnico** a partir del an√°lisis EDA y los requisitos funcionales:

1. **Preprocesamiento:** redimensionamiento, normalizaci√≥n, reducci√≥n de ruido.  
2. **Localizaci√≥n/segmentaci√≥n:** bordes, umbrales adaptativos, morfolog√≠a, o modelo CNN/Detecci√≥n.  
3. **C√°lculos y mediciones:** alineaci√≥n, separaciones, conteo, m√©tricas geom√©tricas.  
4. **Clasificaci√≥n:** OK/Defecto o m√©tricas de calidad.  

**Selecci√≥n de herramientas:**
- M√©todos cl√°sicos (OpenCV, filtros, morfolog√≠a).  
- Deep Learning (CNN, CNN+Transformers) seg√∫n la separabilidad visual detectada (PCA/embeddings).

**Definir KPIs de desempe√±o:** precisi√≥n, recall, F1-score, tiempo de ciclo.  
**Protocolo de pruebas:** FAT (Factory Acceptance Test).  

**Salidas:**  
- Diagrama de bloques del pipeline.  
- Par√°metros iniciales de procesamiento.  
- Dataset curado y dividido (train / val / test).

---

## 8Ô∏è‚É£ Implementar
**Actividades:**
- Integraci√≥n con PLC, robot o HMI (protocolos EtherNet/IP, Profinet, etc.).  
- Calibraciones: intr√≠nseca y mano-ojo si hay medidas en mm.  
- Construcci√≥n mec√°nica, cableado, seguridad y receta de producto.  
- Ejecuci√≥n de **pruebas FAT** en taller con criterios definidos:  
  tasas de falsos ¬± m√°ximas, tiempo de ciclo, estabilidad y repetibilidad.

---

## 9Ô∏è‚É£ Desplegar
**Fase de puesta en marcha:**
- Instalaci√≥n y **SAT (Site Acceptance Test)** en planta.  
- Validaci√≥n en condiciones reales de producci√≥n.  
- Capacitaci√≥n a operadores y personal de mantenimiento.  
- Entrega de documentaci√≥n t√©cnica:  
  par√°metros de configuraci√≥n, planos el√©ctricos/mec√°nicos, *backups* y manuales.  

**Plan de soporte y mejora continua:**
- Registro de im√°genes y logs de inspecci√≥n.  
- Reentrenos peri√≥dicos y ajuste de umbrales adaptativos.  
- Mantenimiento de la iluminaci√≥n, c√°maras y √≥ptica.

---

‚úÖ **Resumen:**  
El proceso completo abarca desde la definici√≥n de la arquitectura hasta el despliegue en planta, asegurando que el sistema de visi√≥n cumpla los requisitos t√©cnicos, productivos y de calidad bajo condiciones reales de operaci√≥n industrial.


### Tipo de modelo utilizado y justificaci√≥n
Se seleccion√≥ una **ResNet-18** con *transfer learning* desde **ImageNet**, por su equilibrio entre precisi√≥n, velocidad y eficiencia en entornos industriales.  
Se reemplaz√≥ la capa final (`fc`) por una densa binaria para clasificaci√≥n y se aplic√≥ *dropout (0.3)* para regularizaci√≥n.

<img width="975" height="373" alt="image" src="https://github.com/user-attachments/assets/62d7545b-96d5-48e5-9c09-00436556dec0" />
**Fuente:** [CS231n ‚Äî Convolutional Neural Networks](https://cs231n.github.io/convolutional-networks/)

### Preprocesamiento aplicado
- Redimensionamiento a 224√ó224 px  
- Normalizaci√≥n RGB con medias de ImageNet  
- Data augmentation: rotaciones ¬±15¬∞, flips, jitter de brillo y contraste  
- Balanceo mediante **WeightedRandomSampler**

### T√©cnicas de optimizaci√≥n empleadas
- Grid Search sobre hiperpar√°metros (`lr`, `batch_size`, `dropout`, `optimizer`)  
- Regularizaci√≥n por *early stopping* y *weight decay*  
- Entrenamiento con **Adam (lr=0.0005)** y validaci√≥n cruzada  

### M√©tricas de evaluaci√≥n seleccionadas
- **Accuracy**, **F1-Score**, **mAP (mean Average Precision)**  
- **mIoU (Intersection over Union)**  
- **FPS (velocidad de inferencia)**

---

## üìà Resultados

| Dataset | Accuracy | F1-Score | mAP | mIoU | FPS |
|----------|-----------|-----------|------|------|------|
| Dataset 1 (100 im√°genes) | 0.89 | 0.90 | 0.91 | 0.76 | 110 |
| Dataset 2 (1000 im√°genes) | **0.944** | **0.945** | **0.984** | **0.870** | **144** |

**Comparaci√≥n:**  
La expansi√≥n del dataset y la optimizaci√≥n de hiperpar√°metros incrementaron la precisi√≥n en **+1.2 %** y la estabilidad del modelo, manteniendo tiempo real (>30 FPS).

**Gr√°ficos:**
**DATASET 1**
<img width="675" height="746" alt="image" src="https://github.com/user-attachments/assets/459e6f7c-1206-4c6f-a187-bbca281666a6" />

**DATASET 2**
<img width="659" height="752" alt="image" src="https://github.com/user-attachments/assets/2c8b1897-42e5-4c0f-8c1d-f1b6290ec4fc" />

---

## ‚öôÔ∏è Instalaci√≥n y Uso

### Requisitos del sistema
- **Python 3.10+**
- GPU compatible con CUDA 11+ (opcional)
- 8 GB de RAM m√≠nimo
- SO: Windows 11 / Ubuntu 22.04

### Instalaci√≥n
```bash
git clone https://github.com/usuario/AVSI.git
cd AVSI
pip install -r requirements.txt
# 1Ô∏è‚É£ Procesar datos
python -m src.data_processing

# 2Ô∏è‚É£ Entrenar el modelo
python -m src.train --epochs 10 --lr 0.0005

# 3Ô∏è‚É£ Evaluar resultados
python -m src.evaluate
[10/10] train_acc=0.94 | val_acc=0.945
Matriz de confusi√≥n y m√©tricas guardadas en /results/metrics/
