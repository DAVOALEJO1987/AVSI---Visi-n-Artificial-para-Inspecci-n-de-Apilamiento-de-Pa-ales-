{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e1c089",
   "metadata": {},
   "source": [
    "# 03 · Modelado — AVSI\n",
    "**Artificial Vision Stacking Inspection** · *2025-10-22*\n",
    "\n",
    "Entrenamiento de un clasificador **ResNet-18 (transfer learning)** para inspección de apilamiento.\n",
    "Carga dataset desde `data/processed/` (train/val/test), entrena, evalúa y guarda el **mejor modelo**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a5c04",
   "metadata": {},
   "source": [
    "## 1. Configuración y dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Rutas\n",
    "ROOT = Path('.').resolve()\n",
    "DATA_PROC = ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "RESULTS_DIR = ROOT / 'results' / 'metrics'\n",
    "FIG_DIR = ROOT / 'results' / 'figures'\n",
    "\n",
    "for d in [MODELS_DIR, RESULTS_DIR, FIG_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('DATA_PROC:', DATA_PROC)\n",
    "print('MODELS_DIR:', MODELS_DIR)\n",
    "print('RESULTS_DIR:', RESULTS_DIR)\n",
    "print('FIG_DIR:', FIG_DIR)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087cc530",
   "metadata": {},
   "source": [
    "## 2. Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289378b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 2,\n",
    "    'epochs': 10,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 3,        # early stopping\n",
    "    'freeze_backbone': True\n",
    "}\n",
    "print(json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69ba631",
   "metadata": {},
   "source": [
    "## 3. Carga de datos (train/val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dir = DATA_PROC / 'train'\n",
    "val_dir   = DATA_PROC / 'val'\n",
    "test_dir  = DATA_PROC / 'test'\n",
    "\n",
    "if not train_dir.exists() or not val_dir.exists() or not test_dir.exists():\n",
    "    raise FileNotFoundError(\"No se encontró la estructura data/processed/{train,val,test}. Ejecuta 02_preprocesamiento primero.\")\n",
    "\n",
    "# Transformaciones (normalización ImageNet)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG['img_size'], CFG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG['img_size'], CFG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "])\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(val_dir, transform=eval_tfms)\n",
    "test_ds  = datasets.ImageFolder(test_dir, transform=eval_tfms)\n",
    "\n",
    "classes = train_ds.classes\n",
    "num_classes = len(classes)\n",
    "print('Clases:', classes, '| num_classes =', num_classes)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG['batch_size'], shuffle=True,  num_workers=CFG['num_workers'])\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CFG['batch_size'], shuffle=False, num_workers=CFG['num_workers'])\n",
    "test_loader  = DataLoader(test_ds,  batch_size=CFG['batch_size'], shuffle=False, num_workers=CFG['num_workers'])\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635b730",
   "metadata": {},
   "source": [
    "## 4. Modelo ResNet-18 (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "if CFG['freeze_backbone']:\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "# Reemplazar la capa final\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, num_classes)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ac891",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b8de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
    "best_val_acc = 0.0\n",
    "best_path = MODELS_DIR / 'best_model.pt'\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, CFG['epochs']+1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['train_acc'].append(tr_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"[{epoch:02d}/{CFG['epochs']}] train_loss={tr_loss:.4f} | train_acc={tr_acc:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({'model_state': model.state_dict(),\n",
    "                    'classes': classes,\n",
    "                    'cfg': CFG}, best_path)\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= CFG['patience']:\n",
    "            print(\"Early stopping activado.\")\n",
    "            break\n",
    "\n",
    "# Guardar histórico\n",
    "pd.DataFrame(history).to_csv(RESULTS_DIR / 'history.csv', index=False)\n",
    "print('Mejor val_acc:', best_val_acc, '| modelo guardado en:', best_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a89b9",
   "metadata": {},
   "source": [
    "## 6. Curvas de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hist = pd.read_csv(RESULTS_DIR / 'history.csv')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist['train_loss'], label='train_loss')\n",
    "plt.plot(hist['val_loss'], label='val_loss')\n",
    "plt.title('Pérdida')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(FIG_DIR / 'loss.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(hist['train_acc'], label='train_acc')\n",
    "plt.plot(hist['val_acc'], label='val_acc')\n",
    "plt.title('Exactitud')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(FIG_DIR / 'accuracy.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0dd9a",
   "metadata": {},
   "source": [
    "## 7. Evaluación en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar mejor modelo\n",
    "ckpt = torch.load(MODELS_DIR / 'best_model.pt', map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels.numpy())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "\n",
    "print('Accuracy (test):', acc)\n",
    "pd.DataFrame(cm, index=classes, columns=classes).to_csv(RESULTS_DIR / 'confusion_matrix.csv')\n",
    "pd.DataFrame(report).to_csv(RESULTS_DIR / 'classification_report.csv')\n",
    "\n",
    "with open(RESULTS_DIR / 'summary.json', 'w') as f:\n",
    "    json.dump({'test_accuracy': acc, 'best_val_acc': float(ckpt.get('best_val_acc', 0.0)),\n",
    "               'classes': classes, 'cfg': ckpt.get('cfg', {})}, f, indent=2)\n",
    "\n",
    "# Mostrar matriz de confusión\n",
    "plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Matriz de confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Real')\n",
    "plt.colorbar()\n",
    "plt.savefig(FIG_DIR / 'confusion_matrix.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36f9d4e",
   "metadata": {},
   "source": [
    "## 8. Inferencia: función de predicción para una imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31478ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "\n",
    "def load_best_model(model_path=MODELS_DIR / 'best_model.pt'):\n",
    "    ckpt = torch.load(model_path, map_location=DEVICE)\n",
    "    model = models.resnet18(weights=None)  # arquitectura base\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, len(ckpt['classes']))\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    model.eval()\n",
    "    return model, ckpt['classes']\n",
    "\n",
    "infer_tfms = transforms.Compose([\n",
    "    transforms.Resize((CFG['img_size'], CFG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def predict_image(img_path, model=None, classes=None):\n",
    "    if model is None or classes is None:\n",
    "        model, classes = load_best_model()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    x = infer_tfms(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x)\n",
    "        probs = torch.softmax(outputs, dim=1).numpy().squeeze()\n",
    "        idx = int(np.argmax(probs))\n",
    "    return classes[idx], float(probs[idx]), {c: float(p) for c, p in zip(classes, probs)}\n",
    "\n",
    "# Ejemplo de uso (comentar/editar la ruta según tu imagen)\n",
    "# pred, conf, dist = predict_image('data/processed/test/good_stack/ejemplo.jpg')\n",
    "# print(pred, conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36adbf",
   "metadata": {},
   "source": [
    "## 9. Próximos pasos\n",
    "- Descongelar capas superiores del backbone y **fine-tuning** (mejorar mAP/IoU si usas detector).\n",
    "- Ajustar `batch_size`, `lr`, `epochs` y regularización.\n",
    "- Registrar métricas adicionales y guardar curvas en `results/figures/`.\n",
    "- Exportar a `best_model.onnx` si planeas despliegue en otras plataformas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
