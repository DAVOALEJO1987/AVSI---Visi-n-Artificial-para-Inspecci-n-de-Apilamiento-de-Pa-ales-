{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c1f318",
   "metadata": {},
   "source": [
    "# 02 · Preprocesamiento — AVSI\n",
    "**Artificial Vision Stacking Inspection** · *2025-10-22*\n",
    "\n",
    "Este notebook implementa el **pipeline de preprocesamiento** para AVSI:\n",
    "- Limpieza y verificación de imágenes.\n",
    "- Redimensionamiento y normalización.\n",
    "- Data augmentation (rotaciones, flips, jitter, brillo/contraste).\n",
    "- División estratificada **train/val/test**.\n",
    "- Exportación a `data/processed/` con `labels.csv`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829eb1d",
   "metadata": {},
   "source": [
    "## 1. Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8445cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "ROOT = Path('.').resolve()\n",
    "DATA_RAW = ROOT / 'data' / 'raw'\n",
    "DATA_PROC = ROOT / 'data' / 'processed'\n",
    "\n",
    "DS_SMALL = DATA_RAW / 'dataset_100'\n",
    "DS_LARGE = DATA_RAW / 'dataset_1000'\n",
    "\n",
    "EXPECTED_CLASSES = ['good_stack', 'bad_stack']  # ajustar según el proyecto\n",
    "\n",
    "IMG_SIZE = (224, 224)          \n",
    "AUG_PER_IMAGE = 1              \n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15                \n",
    "MIN_WIDTH = 32                 \n",
    "MIN_HEIGHT = 32\n",
    "\n",
    "print('RAW:', DATA_RAW)\n",
    "print('PROC:', DATA_PROC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1338663",
   "metadata": {},
   "source": [
    "## 2. Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7650a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "def list_images(folder: Path):\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    return [p for p in folder.rglob('*') if p.suffix.lower() in IMG_EXTS]\n",
    "\n",
    "def read_image(path: Path):\n",
    "    img = cv2.imread(str(path))\n",
    "    return img\n",
    "\n",
    "def is_valid(img):\n",
    "    if img is None:\n",
    "        return False\n",
    "    h, w = img.shape[:2]\n",
    "    return (w >= MIN_WIDTH) and (h >= MIN_HEIGHT)\n",
    "\n",
    "def resize_normalize(img, size=(224,224)):\n",
    "    return cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def to_rgb(img):\n",
    "    if len(img.shape) == 2:\n",
    "        return cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    return img\n",
    "\n",
    "def aug_flip(img):\n",
    "    return cv2.flip(img, 1)\n",
    "\n",
    "def aug_rotate(img, angle=10):\n",
    "    h, w = img.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
    "    return cv2.warpAffine(img, M, (w, h), borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "def aug_jitter(img, alpha=10, beta=10):\n",
    "    out = cv2.convertScaleAbs(img, alpha=1 + (alpha/255.0)*np.random.uniform(-1,1),\n",
    "                                   beta=np.random.uniform(-beta, beta))\n",
    "    return out\n",
    "\n",
    "def make_augs(img):\n",
    "    ops = [lambda x: x, aug_flip, aug_rotate, aug_jitter]\n",
    "    out = [img]\n",
    "    for _ in range(AUG_PER_IMAGE):\n",
    "        op = random.choice(ops[1:])\n",
    "        if op == aug_rotate:\n",
    "            out.append(op(img, angle=random.choice([-12,-8,-5,5,8,12])))\n",
    "        else:\n",
    "            out.append(op(img))\n",
    "    return out\n",
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8fed",
   "metadata": {},
   "source": [
    "## 3. Inventario y selección de origen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcf05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_small = len(list_images(DS_SMALL))\n",
    "count_large = len(list_images(DS_LARGE))\n",
    "print('Imágenes dataset_100:', count_small)\n",
    "print('Imágenes dataset_1000:', count_large)\n",
    "\n",
    "SOURCE_BASE = DS_LARGE if count_large > 0 else DS_SMALL\n",
    "print('Origen seleccionado:', SOURCE_BASE if SOURCE_BASE.exists() else '(no encontrado)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dacaad",
   "metadata": {},
   "source": [
    "## 4. Catálogo de imágenes y etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_catalog(base: Path):\n",
    "    rows = []\n",
    "    for p in list_images(base):\n",
    "        label = p.parent.name if p.parent.name in EXPECTED_CLASSES else 'unknown'\n",
    "        rows.append({'path': str(p), 'fname': p.name, 'label': label})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = build_catalog(SOURCE_BASE) if SOURCE_BASE.exists() else pd.DataFrame()\n",
    "print('Total imágenes:', len(df))\n",
    "display(df.head())\n",
    "print(df['label'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff30505",
   "metadata": {},
   "source": [
    "## 5. Procesamiento, augmentations y preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DIR = DATA_PROC / 'train'\n",
    "VAL_DIR = DATA_PROC / 'val'\n",
    "TEST_DIR = DATA_PROC / 'test'\n",
    "for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    ensure_dir(d)\n",
    "\n",
    "labels_unique = sorted(df['label'].unique().tolist()) if not df.empty else []\n",
    "for d in [TRAIN_DIR, VAL_DIR, TEST_DIR]:\n",
    "    for lbl in labels_unique:\n",
    "        ensure_dir(d / lbl)\n",
    "\n",
    "valid_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    img = read_image(Path(row['path']))\n",
    "    if not is_valid(img):\n",
    "        continue\n",
    "    img = to_rgb(img)\n",
    "    img = resize_normalize(img, IMG_SIZE)\n",
    "    for k, im in enumerate(make_augs(img)):\n",
    "        valid_rows.append({'label': row['label'], 'img': im, 'src': row['path'], 'aug_idx': k})\n",
    "\n",
    "print('Total procesadas (incluye augmentations):', len(valid_rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5543f",
   "metadata": {},
   "source": [
    "## 6. Split estratificado y guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b23e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(valid_rows) == 0:\n",
    "    print('[Aviso] No hay imágenes válidas. Revisa data/raw/.')\n",
    "else:\n",
    "    df_split = pd.DataFrame([{'label': r['label']} for r in valid_rows])\n",
    "    idx_all = np.arange(len(df_split))\n",
    "\n",
    "    # Split test\n",
    "    trainval_idx, test_idx = train_test_split(idx_all, test_size=TEST_SIZE, stratify=df_split['label'], random_state=42)\n",
    "    # Split val\n",
    "    df_trainval = df_split.iloc[trainval_idx]\n",
    "    train_idx, val_idx = train_test_split(trainval_idx, test_size=VAL_SIZE, stratify=df_trainval['label'], random_state=42)\n",
    "\n",
    "    print('Split -> train:', len(train_idx), 'val:', len(val_idx), 'test:', len(test_idx))\n",
    "\n",
    "    records = []\n",
    "    def save_set(indices, base_dir: Path):\n",
    "        for j in indices:\n",
    "            r = valid_rows[j]\n",
    "            label = r['label']\n",
    "            fname = f\"{Path(r['src']).stem}_a{r['aug_idx']}.jpg\"\n",
    "            out_path = base_dir / label / fname\n",
    "            cv2.imwrite(str(out_path), r['img'])\n",
    "            records.append({'split': base_dir.name, 'path': str(out_path), 'label': label, 'fname': fname, 'source': r['src'], 'aug_idx': r['aug_idx']})\n",
    "\n",
    "    save_set(train_idx, TRAIN_DIR)\n",
    "    save_set(val_idx, VAL_DIR)\n",
    "    save_set(test_idx, TEST_DIR)\n",
    "\n",
    "    df_meta = pd.DataFrame(records)\n",
    "    df_meta.to_csv(DATA_PROC / 'labels.csv', index=False)\n",
    "    display(df_meta.head())\n",
    "    print('Guardado en:', DATA_PROC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98890eb4",
   "metadata": {},
   "source": [
    "## 7. Verificación visual simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d62e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'df_meta' in locals() and not df_meta.empty:\n",
    "    counts = df_meta.groupby(['split','label']).size().reset_index(name='n')\n",
    "    plt.figure()\n",
    "    for sp in counts['split'].unique():\n",
    "        total = counts[counts['split']==sp]['n'].sum()\n",
    "        plt.bar(sp, total)\n",
    "    plt.title('Conteo por split')\n",
    "    plt.xlabel('Split')\n",
    "    plt.ylabel('Imágenes')\n",
    "    plt.show()\n",
    "\n",
    "    cls_counts = df_meta.groupby(['label']).size().reset_index(name='n')\n",
    "    plt.figure()\n",
    "    for _, row in cls_counts.iterrows():\n",
    "        plt.bar(row['label'], row['n'])\n",
    "    plt.title('Conteo por clase (total)')\n",
    "    plt.xlabel('Clase')\n",
    "    plt.ylabel('Imágenes')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('[Aviso] No hay df_meta para graficar.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cbc909",
   "metadata": {},
   "source": [
    "## 8. Siguientes pasos\n",
    "- Ajusta `EXPECTED_CLASSES` si tus carpetas de clase difieren.\n",
    "- Modifica `IMG_SIZE`, `AUG_PER_IMAGE`, `TEST_SIZE` y `VAL_SIZE` según rendimiento.\n",
    "- Continua con **03_modelado.ipynb** (carga desde `data/processed/`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
