# âš–ï¸ Consideraciones Ã‰ticas del Proyecto AVSI

Este documento presenta un anÃ¡lisis exhaustivo de las **consideraciones Ã©ticas** inherentes al diseÃ±o, despliegue y operaciÃ³n del sistema **AVSI (Artificial Vision Stacking Inspection)**.  
El anÃ¡lisis se alinea con los principios de **IA Responsable (Responsible AI)**, evaluando los riesgos y proponiendo estrategias de mitigaciÃ³n desde una perspectiva sociotÃ©cnica.

---

## 1ï¸âƒ£ AnÃ¡lisis de Sesgos y GeneralizaciÃ³n

En el contexto de AVSI, el anÃ¡lisis de sesgos se enfoca en los **sesgos tÃ©cnicos y de representaciÃ³n**, que afectan la robustez y fiabilidad del modelo.

### ğŸ”¹ Sesgo de RepresentaciÃ³n
El dataset base fue capturado bajo un conjunto limitado de condiciones (iluminaciÃ³n, planta, configuraciÃ³n de cÃ¡mara).  
Esto genera un **sesgo de muestreo** que limita la capacidad de **generalizaciÃ³n** del modelo ante entornos distintos, fenÃ³meno conocido como *domain shift*.

### ğŸ”¹ Sesgo de MediciÃ³n
Diferencias sistemÃ¡ticas entre los datos de entrenamiento y el entorno real (calibraciÃ³n de cÃ¡mara, compresiÃ³n de video, desenfoque por movimiento) introducen un **sesgo de mediciÃ³n** que puede degradar el rendimiento predictivo.

### ğŸ”¹ Impacto Predictivo
Estos sesgos se manifiestan como un aumento de **falsos positivos (FPR)** o **falsos negativos (FNR)**.  
Por ejemplo, un modelo entrenado solo con buena iluminaciÃ³n podrÃ­a fallar durante turnos nocturnos, reduciendo su confiabilidad.

### ğŸ”¹ Grupos Afectados
- Exceso de falsos positivos â†’ fatiga por alertas y sobrecarga cognitiva en operarios.  
- Exceso de falsos negativos â†’ riesgos para el consumidor y evaluaciones injustas del personal de calidad.

---

## 2ï¸âƒ£ Equidad y Fairness Operativo

Dado que AVSI inspecciona objetos, la **equidad** se redefine como **consistencia operativa del rendimiento**.

### ğŸ”¹ DefiniciÃ³n de Equidad Operativa
Un sistema equitativo mantiene mÃ©tricas estables (precisiÃ³n, F1-score) sin importar turno, lÃ­nea, lote o supervisor.

### ğŸ”¹ MÃ©tricas de EvaluaciÃ³n
Se auditan mÃ©tricas de error (FPR, FNR) de forma desagregada por variables operativas.  
Diferencias notables entre turnos o lÃ­neas indican inequidad operativa.

### ğŸ”¹ Estrategias de MitigaciÃ³n
- **Data Augmentation:** simular variabilidad de dominio (brillo, contraste, ruido).  
- **Muestreo Estratificado:** asegurar representaciÃ³n de condiciones minoritarias o complejas.

---

## 3ï¸âƒ£ Privacidad y Gobernanza de Datos

Los sistemas de visiÃ³n industrial pueden captar datos contextuales del entorno de trabajo, implicando consideraciones de **privacidad laboral**.

### ğŸ”¹ Datos Sensibles Contextuales
Aunque el objetivo es el producto, las cÃ¡maras pueden capturar indirectamente informaciÃ³n sobre los operarios (movimientos, posiciones, hÃ¡bitos).

### ğŸ”¹ MitigaciÃ³n bajo â€œPrivacy by Designâ€
- **TÃ©cnica (MinimizaciÃ³n de Datos):** recorte (*cropping*) o desenfoque de zonas perifÃ©ricas antes de almacenar imÃ¡genes.  
- **Gobernanza (LimitaciÃ³n de PropÃ³sito):** uso exclusivo para inspecciÃ³n de calidad.  
  - Acceso restringido al equipo de MLOps.  
  - Prohibido para fines de supervisiÃ³n o recursos humanos.

---

## 4ï¸âƒ£ Transparencia y Explicabilidad (XAI)

El modelo ResNet-18 ofrece alto rendimiento, pero su naturaleza de **caja negra** requiere estrategias de **explicabilidad**.

### ğŸ”¹ Transparencia
A nivel de interfaz, la app **Streamlit** muestra inferencias en tiempo real, superponiendo la clasificaciÃ³n (â€œCorrectoâ€ / â€œIncorrectoâ€) y permitiendo al operario ver quÃ© analiza el sistema.

### ğŸ”¹ Explicabilidad (Explainable AI)
Durante validaciÃ³n y auditorÃ­a, se aplican tÃ©cnicas *post-hoc* como **Grad-CAM**, que visualizan las zonas relevantes de la imagen usadas por el modelo.  
Esto permite confirmar que las decisiones se basan en caracterÃ­sticas del producto y no en artefactos del fondo.

---

## 5ï¸âƒ£ AnÃ¡lisis de Impacto Social (Stakeholders)

La introducciÃ³n de AVSI impacta de forma diferente a cada grupo involucrado en la cadena de producciÃ³n.

### ğŸ”¹ Impactos Positivos
| Grupo | Impacto |
|--------|----------|
| **Empresa** | Eficiencia productiva, reducciÃ³n de mermas, mejora de calidad. |
| **Consumidor** | Mayor garantÃ­a de calidad en el producto final. |
| **Operarios** | ReducciÃ³n de tareas repetitivas y riesgos ergonÃ³micos. |

### ğŸ”¹ Impactos Negativos / Riesgos SociotÃ©cnicos
- **AutomatizaciÃ³n de tareas:** requiere **reentrenamiento** del personal (de inspector a supervisor de IA).  
- **Ansiedad por vigilancia:** percepciÃ³n de monitoreo constante que puede afectar el clima laboral.  
- **Complacencia automatizada:** exceso de confianza en el sistema, reduciendo la vigilancia humana ante posibles errores.

---

## 6ï¸âƒ£ Responsabilidad y â€œHuman-in-the-Loopâ€ (HITL)

La responsabilidad se distribuye entre el equipo tÃ©cnico y la gestiÃ³n operativa.

### ğŸ”¹ AsignaciÃ³n de Responsabilidad
- **Equipo de IA:** validaciÃ³n, robustez, documentaciÃ³n.  
- **Gerencia de planta:** implementaciÃ³n, protocolos y capacitaciÃ³n.

### ğŸ”¹ Mecanismos de Accountability
- **Human-in-the-Loop (HITL):** el operario supervisa el sistema y valida las alertas.  
- **MLOps y Gobernanza:** monitoreo continuo del modelo (detecciÃ³n de *model drift*) y retroalimentaciÃ³n para reentrenamiento con datos curados.

---

## 7ï¸âƒ£ Uso Dual y Riesgos Ã‰tico-Laborales

El riesgo de â€œuso dualâ€ se refiere al **empleo indebido** del sistema con fines no previstos.

### ğŸ”¹ Riesgo de Mal Uso
El sistema podrÃ­a usarse para **vigilar la productividad de los empleados**, lo cual constituirÃ­a una violaciÃ³n Ã©tica.

### ğŸ”¹ Estrategias de Salvaguarda
- **Controles TÃ©cnicos:** anonimizaciÃ³n y privacidad por defecto.  
- **Controles de Gobernanza:** polÃ­ticas internas que prohÃ­ben el uso del sistema para evaluaciÃ³n del desempeÃ±o personal.

---

## 8ï¸âƒ£ Limitaciones Reconocidas y Robustez

### ğŸ”¹ Datos Fuera de DistribuciÃ³n (OOD)
El modelo no es confiable para productos o empaques distintos a los del entrenamiento.  
Se requiere reentrenamiento o validaciÃ³n antes de cambios de diseÃ±o.

### ğŸ”¹ Perturbaciones del Dominio
El rendimiento puede degradarse con variaciones ambientales (iluminaciÃ³n, sombras, velocidad de cinta).

### ğŸ”¹ Casos LÃ­mite
- Oclusiones parciales (mano u objeto tapando el producto).  
- Presentaciones anÃ³malas del paquete (giro, posiciÃ³n lateral).

### âš ï¸ Advertencia de Uso
El sistema AVSI es una herramienta de **asistencia a la inspecciÃ³n**, no un reemplazo total del juicio humano.  
La supervisiÃ³n humana cualificada sigue siendo indispensable para garantizar la calidad del producto.

---

## ğŸ“š Referencia de Principios Ã‰ticos

1. **IA Responsable (RAI)** â€“ IEEE Global Initiative on Ethics of Autonomous Systems.  
2. **Privacy by Design** â€“ Cavoukian, A. (2010).  
3. **Human-in-the-Loop Frameworks** â€“ Amershi et al. (2019).  
4. **XAI â€“ Explainable Artificial Intelligence** â€“ DARPA Initiative (2018).  
5. **Ethical AI in Industrial Automation** â€“ OECD AI Principles (2021).

<img width="930" height="1395" alt="image" src="https://github.com/user-attachments/assets/025e2581-8230-4e3f-9466-d406a3e4ac2d" />

**Fuente:** Generado por ChatGPT
